{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os, logging, re, gc, time\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from karateclub.community_detection.non_overlapping import GEMSEC, EdMot, SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- read original file -------------\n",
    "def process_raw_data(filesName, _path):\n",
    "    data = pd.read_csv(_path+filesName+'.csv', index_col=False, dtype=str)\n",
    "    # ====================\n",
    "    # ----------- remain only english data & tweet+hashtag ------------\n",
    "    data = data[data['lang'].apply(lambda x: x=='en')]\n",
    "    logging.info(\"number of english data : \", str(len(data)))\n",
    "    data_with_hashtag = data[data['hashtag'].apply(lambda x: type(x)==str and len(x)>2)]\n",
    "    logging.info(\"number of eng+hashtag : \", str(len(data_with_hashtag)))\n",
    "    del data\n",
    "    # ====================\n",
    "    # ---- remove unnecesery columns --------------\n",
    "    data_with_hashtag.drop(['url', 'replyCount', 'lang','retweetCount', 'likeCount',\n",
    "                            'quoteCount', 'conversationId','mentionedUsers', 'time'],axis=1, inplace=True)\n",
    "    #=====================\n",
    "    # ------------- preprocessing -----------------\n",
    "    url_pattern = r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n",
    "    url_pattern2 = r'((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)'\n",
    "\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['content'].str.replace(r'@\\w+', '')\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(url_pattern2, '')\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('#', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('()', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('(', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(')', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(':', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('\\n', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('\\r', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('!', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('&', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('*', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(r'\\d+', ' ')\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('$', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('-', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(':', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(',', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(';', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(\"'\", ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(\".\", ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('\"', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('  ', ' ', regex=False)\n",
    "    data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace('  ', ' ', regex=False)\n",
    "    # ===================\n",
    "    data_with_hashtag.to_csv(_path+\"cleaned_\"+filesName+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gensim_model():\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('/hdd/crawl-300d-2M.vec')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tweet_vec(main_df):\n",
    "    tweet_vec = list()\n",
    "    status_tweet = list()\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    for i in range(len(main_df)):\n",
    "        tweet_matrix = list()\n",
    "        tweet = main_df.iloc[i]['cleaned']\n",
    "\n",
    "        tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "        for word in tweet_tokens:\n",
    "            if word not in stopwords_english:\n",
    "                try:\n",
    "                    tweet_matrix.append(model[word].tolist())\n",
    "                except:\n",
    "                    #print(word)\n",
    "                    pass\n",
    "        if len(tweet_matrix):\n",
    "            tweet_vec.append(np.array(tweet_matrix).mean(axis=0))\n",
    "            status_tweet.append(\"good\")\n",
    "        else:\n",
    "            tweet_vec.append(-1)\n",
    "            status_tweet.append(\"bad\")\n",
    "    \n",
    "    return tweet_vec, status_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_min_max_median(val):\n",
    "    ll = list()\n",
    "    for i in range(val):\n",
    "        for j in range(i+1, val):\n",
    "            ll.append(dist[i,j])\n",
    "    print(\"max : \",max(ll))\n",
    "    print(\"min : \",min(ll))\n",
    "    _median = np.median(np.array(ll))\n",
    "    print(\"median : \",_median)\n",
    "    return _median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_unique_hashtag(df):\n",
    "    hashtags = list()\n",
    "    hashtags_tmp = df['hashtag'].values.tolist()\n",
    "    for i in hashtags_tmp:\n",
    "        hashtags.append(i.replace(\" \", \"\").split(\",\"))\n",
    "    hashtags = list(chain(*hashtags))\n",
    "    print(\"all hashtags :\", len(hashtags))\n",
    "\n",
    "    unique_hashtag = list(set(hashtags))\n",
    "    print(\"unique hashtags :\", len(unique_hashtag))\n",
    "    \n",
    "    ids = df['id'].values.tolist()\n",
    "    print(\"all ids:\", len(ids))\n",
    "    \n",
    "    return ids, unique_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(Graph, ids,  unique_hashtag, df, i_iloc, j_iloc):\n",
    "    #----------- add ids nodes -------------\n",
    "    for _id in ids:\n",
    "        Graph.add_node(_id)\n",
    "    print(\"1- nodes : \", Graph.number_of_nodes())\n",
    "#     --------- add hashtag nodes -----------\n",
    "    for _hashtag in unique_hashtag:\n",
    "        Graph.add_node(_hashtag)\n",
    "    print(\"2- nodes : \", Graph.number_of_nodes())\n",
    "#     --------- add edges between hashtag and id -----------\n",
    "    for _, i in df.iterrows():\n",
    "        for _h in i['hashtag'].replace(\" \",\"\").split(\",\"):\n",
    "            Graph.add_edge(i['id'], _h, weight=1) # ---------------------- < -----  hsahtag_weight=1\n",
    "    print(\"just id-hashtag edges : \", Graph.number_of_edges())\n",
    "#     --------- add edges between id and id with weight -----------\n",
    "    f = open(_path_original_files + 'edges', 'w')\n",
    "    for ii, jj in zip(i_iloc, j_iloc):\n",
    "        if (ii != jj and jj < ii):\n",
    "            print(translator[ii], translator[jj], dist[ii, jj], file=f)\n",
    "            w = 2 - dist[ii, jj] # --------------------------------------- < ------ max_idid_weight=2\n",
    "            Graph.add_edge(df.iloc[ii]['id'],df.iloc[jj]['id'],weight=w) \n",
    "    f.close()\n",
    "    print(\"all edges : \", Graph.number_of_edges())\n",
    "    print(\"isoated nodes : \", list(nx.isolates(Graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort the list of tuples by its second item <---* Update\n",
    "def Sort_Tuple(tup):\n",
    "    lst = len(tup)  \n",
    "    for i in range(0, lst):  \n",
    "        for j in range(0, lst-i-1):  \n",
    "            if (tup[j][1] > tup[j + 1][1]):  \n",
    "                temp = tup[j]  \n",
    "                tup[j]= tup[j + 1]  \n",
    "                tup[j + 1]= temp  \n",
    "    return tup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 5 files ...  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43868/3228071623.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_with_hashtag['cleaned'] = data_with_hashtag['content'].str.replace(r'@\\w+', '')\n",
      "/tmp/ipykernel_43868/3228071623.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(url_pattern2, '')\n",
      "/tmp/ipykernel_43868/3228071623.py:32: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(r'\\d+', ' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'hadn', 't', 'this', 'shan', 'your', 'through', 'how', 'these', 'or', 'same', 'under', 'out', 'own', 'for', \"wasn't\", 'am', 'at', 'so', 'yours', 'had', 'whom', 'ma', 'should', 'doesn', \"you'll\", 's', \"weren't\", 'wasn', 'don', 'being', \"couldn't\", 'both', 'an', 'been', 've', 'in', 'it', 'themselves', 'again', 'he', 'which', 'has', 'theirs', \"isn't\", \"hadn't\", 'itself', 'of', 'there', \"needn't\", 'more', 'here', 'from', 'is', 'once', 'all', 'after', \"she's\", 'herself', 'until', 'his', \"doesn't\", 'mightn', \"you'd\", 'than', 'no', 'further', 'each', 'why', 'other', 'against', 'such', 'its', 'when', 'can', 'd', 'a', 'only', 'not', 'nor', 'what', 'because', 'by', 'just', \"mightn't\", 'having', 'their', 're', 'few', \"don't\", 'were', 'ain', 'most', 'during', 'myself', 'about', \"mustn't\", 'her', 'above', \"you've\", 'ourselves', 'that', 'then', 'while', 'hers', 'do', 'yourself', 'but', 'as', 'into', \"aren't\", \"shan't\", 'won', 'couldn', 'didn', 'have', \"it's\", 'too', 'was', 'if', 'o', 'yourselves', 'shouldn', \"should've\", 'off', 'isn', 'on', 'our', 'hasn', 'does', 'm', 'them', 'below', 'aren', 'to', 'with', 'will', 'she', 'did', \"didn't\", 'doing', 'me', 'you', \"you're\", \"wouldn't\", 'and', 'be', 'who', 'down', 'where', 'my', 'll', 'i', \"shouldn't\", 'we', 'over', 'some', \"that'll\", 'needn', 'him', \"won't\", 'are', 'very', \"hasn't\", 'haven', 'mustn', 'before', 'up', 'ours', 'they', 'himself', \"haven't\", 'wouldn', 'those', 'any', 'between', 'now', 'y', 'weren', 'the'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mahdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "_path_original_files = '/home/mahdi/Desktop/nc-final-project/files_twitter/'\n",
    "fileNames = [\"near_washington_1015_1016_tweets\", \"near_washington_1016_1017_tweets\",\n",
    "             \"near_washington_1017_1018_tweets\", \"near_washington_1018_1019_tweets\",\n",
    "             \"near_washington_1019_1022_tweets\"]\n",
    "print(\"Cleaning \"+str(len(fileNames))+\" files ... \", end=\" \")\n",
    "for f in fileNames[:1]:\n",
    "    process_raw_data(f, _path_original_files)\n",
    "print(\"done\")\n",
    "model = load_gensim_model()\n",
    "nltk.download('stopwords')\n",
    "stopwords_english = set(stopwords.words('english'))\n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43868/1153684837.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_with_hashtag['cleaned'] = data_with_hashtag['content'].str.replace(r'@\\w+', '')\n",
      "/tmp/ipykernel_43868/1153684837.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(url_pattern2, '')\n",
      "/tmp/ipykernel_43868/1153684837.py:32: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_with_hashtag['cleaned'] = data_with_hashtag['cleaned'].str.replace(r'\\d+', ' ')\n"
     ]
    }
   ],
   "source": [
    "for f in fileNames[:1]:\n",
    "    process_raw_data(f, _path_original_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,_, f in os.walk(_path_original_files):\n",
    "    pass\n",
    "f = [f'cleaned_{i}.csv' for i in fileNames]\n",
    "f = ['cleaned_near_washington_1015_1016_tweets.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaned_near_washington_1015_1016_tweets.csv']\n",
      "=.==.==.==.==.==.==.==.==.==.=\n",
      "cleaned_near_washington_1015_1016_tweets.csv\n",
      "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
      "33074\n",
      "max :  8.638947300732344\n",
      "min :  0.0\n",
      "median :  2.132292216232245\n"
     ]
    }
   ],
   "source": [
    "#fileName = 'cleaned_near_washington_1018_1019_tweets.csv'\n",
    "print(f)\n",
    "fileName = f[0]\n",
    "# for fileName in f:\n",
    "print(10*\"=.=\")\n",
    "print(fileName)\n",
    "main_df = pd.read_csv(_path_original_files+fileName, index_col=False, dtype=str)\n",
    "\n",
    "grp_maindf = main_df.groupby('date')\n",
    "_, idate = list(grp_maindf)[0]\n",
    "# for _, idate in grp_maindf:\n",
    "t0 = time.time()\n",
    "print(10*'-.-.')\n",
    "df = idate\n",
    "#df = main_df\n",
    "df = df.reset_index(drop=True)\n",
    "_date = df.iloc[0]['date']\n",
    "print(df.shape[0])\n",
    "all_tweet_vec, all_status_tweet = cal_tweet_vec(df)\n",
    "_df_tmp = pd.DataFrame()\n",
    "_df_tmp['vec'] = all_tweet_vec\n",
    "_df_tmp['status'] = all_status_tweet\n",
    "tweet_vec = _df_tmp[_df_tmp['status']=='good']['vec'].values.tolist()\n",
    "dist = sklearn.metrics.pairwise_distances(tweet_vec, tweet_vec, n_jobs=8)\n",
    "dist = np.array(dist)\n",
    "_median = cal_min_max_median(round(dist.shape[0]/2))\n",
    "#i_iloc, j_iloc = np.where(dist<_median/4)\n",
    "i_iloc, j_iloc = np.where(dist<_median/3)\n",
    "# i_iloc, j_iloc = np.where(dist<_median/2)\n",
    "# i_iloc, j_iloc = np.where(dist<_median)\n",
    "dfgood = df[_df_tmp['status']=='good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 edges removed\n",
      "117 edges added\n",
      "104 edges different weight\n",
      "8329 common\n"
     ]
    }
   ],
   "source": [
    "translator = {}\n",
    "df_all = pd.read_csv(_path_original_files+fileName[len('cleaned_'):], index_col=False, dtype=str)\n",
    "for i in range(len(dfgood)):\n",
    "    translator[i] = int(dfgood.iloc[i]['Unnamed: 0'])\n",
    "edges_original = {}\n",
    "for ii, jj in zip(i_iloc, j_iloc):\n",
    "    if ii == jj:\n",
    "        continue\n",
    "    a = translator[int(ii)]\n",
    "    b = translator[int(jj)]\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "    edges_original[(a, b)] = dist[ii, jj]\n",
    "            \n",
    "edges_cpp = {(int(line.split()[0]), int(line.split()[1])): float(line.split()[2]) for line in open(_path_original_files + 'edges_c').readlines()}\n",
    "edges_removed = set(edges_original) - set(edges_cpp)\n",
    "edges_added = set(edges_cpp) - set(edges_original)\n",
    "print(len(edges_removed), 'edges removed')\n",
    "print(len(edges_added), 'edges added')\n",
    "eps = 1e-3\n",
    "weight_different = [k for k in (set(edges_cpp).intersection(edges_original)) if abs(edges_cpp[k] - edges_original[k]) > eps][1:]\n",
    "print(len(weight_different), 'edges have different weight')\n",
    "print(len(set(edges_cpp).intersection(edges_original)), 'common')\n",
    "\n",
    "\n",
    "#del model, _df_tmp, tweet_vec, all_status_tweet, all_tweet_vec, df\n",
    "#gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
